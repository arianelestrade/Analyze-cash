{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import et processing des données des différentes requêtes - partie 1 cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectif : cleaner la raw data obtenue via Twint (située dans le dossier data/make_queries) pour nettoyer le bruit (ie. tweets non pertinent par rapport à notre problématique sur le cash ou les moyens de paiement scripturaux).\n",
    "Pour l'instant, ce script ne traite que les tweets issus de la requête francophone cash et cb. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import random\n",
    "import re \n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from datetime import date\n",
    "\n",
    "pd.options.display.max_colwidth = None\n",
    "pd.options.display.max_columns = 30\n",
    "\n",
    "from utils import query_to_words\n",
    "from utils import score_language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-11\n"
     ]
    }
   ],
   "source": [
    "WORDS_ENGLISH = set(\"of will he she them they it are is be over and to can can't\".split())\n",
    "WORDS_FRENCH = set(\"de du le la les un une des il elle ils elles on nous vous moi me ma te tu toi son sa celles ceux notre nos votre vos leurs leur comment dans mais car ou et pour en par à au est sont a ont peut peuvent\".split())\n",
    "query_fr = '(espece OR especes OR espèce OR espèces OR billet OR billets OR piece OR pieces OR pièce OR pièces OR monnaie OR cash OR liquide OR retraits OR retrait OR distributeur OR distributeurs OR \" DAB \" OR guichet OR guichets OR carte OR cartes OR \" CB \" OR \"sans contact\" OR \" visa \" OR mastercard OR \"cash back\" OR \" NFC \" OR \"Google Pay\" OR \"ApplePay\" OR \"Paylib\" OR \"Lydia\" OR \"Lyf Pay\" OR \"Alipay\" OR \"Samsung Pay\" OR \"Stocard Pay\" OR \"paiements mobiles\" OR \"chèques\" OR chèque OR cheque OR cheques) AND (paiement OR paiements OR payer OR reglement OR reglements OR réglement OR réglements OR regler OR régler OR achat OR achats OR acheter OR retirer OR virement OR virements OR virer OR depense OR dépense OR dépenses OR depenses OR dépenser OR depenser)'\n",
    "borne_temp = str(date.today()) #\"2021-02-01\"\n",
    "print(borne_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Requête francophone cash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"/home/cash/data/queries/query_fr_cash.csv\")\n",
    "mask_dict = {\n",
    "    \"billet_transport\": \"sncf| tgv | ter |train|avion|ratp|navigo|bus|lufthanza|easyjet|metro|gare du|gare de\", # sert pour \"billet\" MAIS aussi pour \"guichet\"\n",
    "    \"billet_event\": \"foot|penalty|matchs?|psg|concerts?|arenum|tournee|billeterie|olympia|stade de france|festival|spectacle|congres\",\n",
    "    \"vetements\": \"zara|la piece|vetement|piece prefere|costume|pieces? mode|pieces? vintage| taille |fringue|collection\",\n",
    "    \"composants\": \"pieces? detache|pieces? rapporte|pieces? de rechange|changement de piece?|piece separe|remplacement de piece?\" +\n",
    "                  \"|voiture|peugeot|renault|automobile|pieces? de la maison|pieces? de maison\",\n",
    "    \"manga\": \"on\\w piece|tomes?\",\n",
    "    \"expression_piece\": \"de tte piece|de toute piece|€ piece|euros? piece|piece d'?identite\",\n",
    "    \"crypto\": \"crypto|bitcoin|ethereum|eth|\\w+coin|blockchain\", # probablement à conserver\n",
    "    \"argot\": \"wsh|wesh|m+d+r+|p+t+d+r+|de ouf|frr+|shab|bolos+|bouf+on|hagar|zebi|frerot?\",\n",
    "    \"expression_retrait\": \"retrait magasin|retrait en magasin|retrait express|retrait de commandes?|droit de retrait|retrait de points?\",\n",
    "    \"jeux_video\": \"console|playstation|xbox|fortnite|micromania|iphone|ps[0-9]\", # à conserver\n",
    "    \"misc\": \"liquide vaisselle|radio dab|radiodab|cash back|cash out|cash flow|comme dab|espece de|regler le probleme|air ?liquide\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modification de df : colonnes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean la variable textuelle\n",
    "df[\"clean_tweet\"] =  df.tweet.str.lower().str.replace(\"\\.|:|,|'|’\", \" \").str.replace(r'http\\S+', '').str.replace(r't co+', '')\n",
    "\n",
    "# Calcule score de pertinence des tweets : plus le score est élevé, plus le tweet est pertinent. \n",
    "words = query_to_words(query_fr)\n",
    "df[\"pertinence_score\"] = df.clean_tweet.str.count(pat=\"|\".join(words))\n",
    "df.sort_values(\"pertinence_score\", inplace=True, ascending=False)\n",
    "    \n",
    "# Calcule score pour déterminer si le tweet est en anglais ou pas : si 1 ou plus, tweet anglais.  \n",
    "df[\"french_score\"] = df.clean_tweet.apply(lambda string: score_language(string,WORDS_FRENCH))\n",
    "df[\"english_score\"] = df.clean_tweet.apply(lambda string: score_language(string,WORDS_ENGLISH))\n",
    "    \n",
    "# Ajoute variable temporelle datetime\n",
    "df[\"datetime\"] = pd.to_datetime(df.date + \" \" + df.time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modification de df : filtrage des observations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    billet_transport: 16.9%\n",
      "        billet_event: 4.0%\n",
      "           vetements: 0.8%\n",
      "          composants: 1.1%\n",
      "               manga: 1.6%\n",
      "    expression_piece: 0.0%\n",
      "              crypto: 2.3%\n",
      "               argot: 3.7%\n",
      "  expression_retrait: 0.6%\n",
      "          jeux_video: 1.1%\n",
      "                misc: 1.1%\n",
      "Cleaning total: 114330 lignes retirées sur 314539\n",
      "Cleaning des tweets non francophones: 1837 lignes retirées sur 202046\n"
     ]
    }
   ],
   "source": [
    "# Filtrer les tweets du dernier mois (pas encore toutes les données)\n",
    "shape_avant = df.shape[0]\n",
    "df = df.loc[df[\"date\"] < borne_temp]\n",
    "    \n",
    "# Filtrer les tweets HS grâce aux règles métiers\n",
    "df = df[df.pertinence_score > 0]\n",
    "masks_series = {\n",
    "    label: df.clean_tweet.str.contains(value) \n",
    "    for label, value in mask_dict.items()\n",
    "}\n",
    "for (label, mask) in masks_series.items():\n",
    "    print(f\"{label.rjust(20)}: {round(100 * mask.mean(), 1)}%\")\n",
    "df = df[~ reduce(lambda x, y: x | y, masks_series.values())]\n",
    "    \n",
    "# Filtrer les tweets autres que francophones\n",
    "df = df[df.english_score < 1]\n",
    "shape_inter = df.shape[0]\n",
    "df = df[df.french_score>= 1]\n",
    "shape_apres = df.shape[0]\n",
    "\n",
    "print(f\"Cleaning total: {shape_avant - shape_apres} lignes retirées sur {shape_avant}\")\n",
    "print(f\"Cleaning des tweets non francophones: {shape_inter - shape_apres} lignes retirées sur {shape_inter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.isnull().values.any())\n",
    "print(df.clean_tweet.isnull().values.any())\n",
    "df.created_at.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200209"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(r\"/home/cash/output/cleaned_queries/without_geoloc/query_fr_cash_cleaned.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200209"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.read_pickle(f\"/home/cash/output/cleaned_queries/without_geoloc/query_fr_cash_cleaned.pickle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Requête francophone CB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"/home/cash/data/queries/query_fr_cb.csv\")\n",
    "mask_dict = {\n",
    "    \"carte\": \"jeu de cartes?|cartes?  ?cadeaux?|carte  ?m(e|è)re|cartes? de biblioth(è|e)que|cartes?  ?graphiques?|cartes?  ?vitales?|cartes?  ?grises?|le cb|cartes? m(e|é)moires?|cartes?  ?culture|carte visa|cartes?  ?d ?identit(e|é)|cartes?  ?de  ?s(é|e)jours?|cartes?  ?blanches?|concours|cartes?  ?de  ?presse|tirages?  ?au  ?sort|cartes?  ?jaunes?|cartes?  ?sur  ?table\",\n",
    "    \"cb\" : \"d(é|e)pens(er|é|e) cb\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean la variable textuelle\n",
    "df[\"clean_tweet\"] =  df.tweet.str.lower().str.replace(\"\\.|:|,|'|’\", \" \").str.replace(r'http\\S+', '').str.replace(r't co+', '')\n",
    "\n",
    "# Calcule score de pertinence des tweets : plus le score est élevé, plus le tweet est pertinent. \n",
    "words = query_to_words(query_fr)\n",
    "df[\"pertinence_score\"] = df.clean_tweet.str.count(pat=\"|\".join(words))\n",
    "df.sort_values(\"pertinence_score\", inplace=True, ascending=False)\n",
    "    \n",
    "# Calcule score pour déterminer si le tweet est en anglais ou pas : si 1 ou plus, tweet anglais.  \n",
    "df[\"french_score\"] = df.clean_tweet.apply(lambda string: score_language(string,WORDS_FRENCH))\n",
    "df[\"english_score\"] = df.clean_tweet.apply(lambda string: score_language(string,WORDS_ENGLISH))\n",
    "    \n",
    "# Ajoute variable temporelle datetime\n",
    "df[\"datetime\"] = pd.to_datetime(df.date + \" \" + df.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cash/.local/lib/python3.8/site-packages/pandas/core/strings.py:2001: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  return func(self, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               carte: 11.1%\n",
      "                  cb: 0.1%\n",
      "Cleaning total: 38715 lignes retirées sur 294417\n",
      "Cleaning des tweets non francophones: 1981 lignes retirées sur 257683\n"
     ]
    }
   ],
   "source": [
    "# Filtrer les tweets du dernier mois (pas encore toutes les données)\n",
    "shape_avant = df.shape[0]\n",
    "df = df.loc[df[\"date\"] < borne_temp]\n",
    "    \n",
    "# Filtrer les tweets HS grâce aux règles métiers\n",
    "df = df[df.pertinence_score > 0]\n",
    "masks_series = {\n",
    "    label: df.clean_tweet.str.contains(value) \n",
    "    for label, value in mask_dict.items()\n",
    "}\n",
    "for (label, mask) in masks_series.items():\n",
    "    print(f\"{label.rjust(20)}: {round(100 * mask.mean(), 1)}%\")\n",
    "df = df[~ reduce(lambda x, y: x | y, masks_series.values())]\n",
    "    \n",
    "# Filtrer les tweets autres que francophones\n",
    "df = df[df.english_score < 1]\n",
    "shape_inter = df.shape[0]\n",
    "df = df[df.french_score>= 1]\n",
    "shape_apres = df.shape[0]\n",
    "\n",
    "print(f\"Cleaning total: {shape_avant - shape_apres} lignes retirées sur {shape_avant}\")\n",
    "print(f\"Cleaning des tweets non francophones: {shape_inter - shape_apres} lignes retirées sur {shape_inter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "255702"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.isnull().values.any())\n",
    "print(df.clean_tweet.isnull().values.any())\n",
    "df.created_at.isnull().values.any()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(r\"/home/cash/output/cleaned_queries/without_geoloc/query_fr_cb_cleaned.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open('filename.pickle', 'wb') as handle:\n",
    "    pickle.dump(a, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('filename.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255702\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(\"/home/cash/output/cleaned_queries/without_geoloc/query_fr_cb_cleaned.pickle\")\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Requete francophone sans-contact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44459, 36)\n",
      "2021-04-30\n",
      "(56777, 36)\n",
      "2021-04-30\n"
     ]
    }
   ],
   "source": [
    "df_contactless_cb_fr = pd.read_csv(f\"/home/cash/data/queries/query_contactless_cb_fr_part1.csv\")\n",
    "print(df_contactless_cb_fr.shape)\n",
    "print(max(df_contactless_cb_fr['date']))\n",
    "df_contactless_smartphone_fr = pd.read_csv(f\"/home/cash/data/queries/query_contactless_smartphone_fr_part1.csv\")\n",
    "print(df_contactless_smartphone_fr.shape)\n",
    "print(max(df_contactless_smartphone_fr['date']))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#relancement de la requete et qu'un csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44459, 37)\n",
      "(56777, 37)\n"
     ]
    }
   ],
   "source": [
    "df_contactless_smartphone_fr['type_contactless'] = 'smartphone'\n",
    "df_contactless_cb_fr['type_contactless'] = 'cb'\n",
    "\n",
    "print(df_contactless_cb_fr.shape)\n",
    "print(df_contactless_smartphone_fr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101236, 37)\n"
     ]
    }
   ],
   "source": [
    "df_contactless = pd.concat([df_contactless_cb_fr, df_contactless_smartphone_fr])\n",
    "print(df_contactless.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agrégation des DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_dict = {\n",
    "    \"billet_transport\": \"sncf| tgv | ter |train|avion|ratp|navigo|bus|lufthanza|easyjet|metro|gare du|gare de\", # sert pour \"billet\" MAIS aussi pour \"guichet\"\n",
    "    \"billet_event\": \"foot|penalty|matchs?|psg|concerts?|arenum|tournee|billeterie|olympia|stade de france|festival|spectacle|congres\",\n",
    "    \"vetements\": \"zara|la piece|vetement|piece prefere|costume|pieces? mode|pieces? vintage| taille |fringue|collection\",\n",
    "    \"composants\": \"pieces? detache|pieces? rapporte|pieces? de rechange|changement de piece?|piece separe|remplacement de piece?\" +\n",
    "                  \"|voiture|peugeot|renault|automobile|pieces? de la maison|pieces? de maison\",\n",
    "    \"manga\": \"on\\w piece|tomes?\",\n",
    "    \"expression_piece\": \"de tte piece|de toute piece|€ piece|euros? piece|piece d'?identite\",\n",
    "    \"crypto\": \"crypto|bitcoin|ethereum|eth|\\w+coin|blockchain\", # probablement à conserver\n",
    "    \"argot\": \"wsh|wesh|m+d+r+|p+t+d+r+|de ouf|frr+|shab|bolos+|bouf+on|hagar|zebi|frerot?\",\n",
    "    \"expression_retrait\": \"retrait magasin|retrait en magasin|retrait express|retrait de commandes?|droit de retrait|retrait de points?\",\n",
    "    \"jeux_video\": \"console|playstation|xbox|fortnite|micromania|iphone|ps[0-9]\", # à conserver\n",
    "    \"misc\": \"liquide vaisselle|radio dab|radiodab|cash back|cash out|cash flow|comme dab|espece de|regler le probleme|air ?liquide\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modification des colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean la variable textuelle\n",
    "df_contactless[\"clean_tweet\"] =  df_contactless.tweet.str.lower().str.replace(\"\\.|:|,|'|’\", \" \").str.replace(r'http\\S+', '').str.replace(r't co+', '')\n",
    "\n",
    "# Calcule score de pertinence des tweets : plus le score est élevé, plus le tweet est pertinent. \n",
    "words = query_to_words(query_fr)\n",
    "df_contactless[\"pertinence_score\"] = df_contactless.clean_tweet.str.count(pat=\"|\".join(words))\n",
    "df_contactless.sort_values(\"pertinence_score\", inplace=True, ascending=False)\n",
    "    \n",
    "# Calcule score pour déterminer si le tweet est en anglais ou pas : si 1 ou plus, tweet anglais.  \n",
    "df_contactless[\"french_score\"] = df_contactless.clean_tweet.apply(lambda string: score_language(string,WORDS_FRENCH))\n",
    "df_contactless[\"english_score\"] = df_contactless.clean_tweet.apply(lambda string: score_language(string,WORDS_ENGLISH))\n",
    "    \n",
    "# Ajoute variable temporelle datetime\n",
    "df_contactless[\"datetime\"] = pd.to_datetime(df_contactless.date + \" \" + df_contactless.time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modification et filtrage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    billet_transport: 3.5%\n",
      "        billet_event: 0.7%\n",
      "           vetements: 0.1%\n",
      "          composants: 0.2%\n",
      "               manga: 1.0%\n",
      "    expression_piece: 0.0%\n",
      "              crypto: 2.8%\n",
      "               argot: 0.9%\n",
      "  expression_retrait: 0.0%\n",
      "          jeux_video: 2.4%\n",
      "                misc: 0.1%\n"
     ]
    }
   ],
   "source": [
    "# Filtrer les tweets du dernier mois (pas encore toutes les données)\n",
    "shape_avant = df_contactless.shape[0]\n",
    "df_contactless = df_contactless.loc[df_contactless[\"date\"] < borne_temp]\n",
    "\n",
    "# Filtrer les tweets HS grâce aux règles métiers\n",
    "df_contactless = df_contactless[df_contactless.pertinence_score > 0]\n",
    "masks_series = {\n",
    "    label: df_contactless.clean_tweet.str.contains(value) \n",
    "    for label, value in mask_dict.items()\n",
    "}\n",
    "for (label, mask) in masks_series.items():\n",
    "    print(f\"{label.rjust(20)}: {round(100 * mask.mean(), 1)}%\")\n",
    "df_contactless = df_contactless[~ reduce(lambda x, y: x | y, masks_series.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning total: 53206 lignes retirées sur 101236\n",
      "Cleaning des tweets non francophones: 18083 lignes retirées sur 66113\n"
     ]
    }
   ],
   "source": [
    "# Filtrer les tweets autres que francophones\n",
    "df_contactless = df_contactless[df_contactless.english_score < 1]\n",
    "shape_inter = df_contactless.shape[0]\n",
    "df_contactless = df_contactless[df_contactless.french_score>= 1]\n",
    "shape_apres = df_contactless.shape[0]\n",
    "\n",
    "print(f\"Cleaning total: {shape_avant - shape_apres} lignes retirées sur {shape_avant}\")\n",
    "print(f\"Cleaning des tweets non francophones: {shape_inter - shape_apres} lignes retirées sur {shape_inter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                    clean_tweet  \\\n",
      "25636  @megaviria bonjour  la carte lydia est une carte de paiement mastercard conçue par lydia   comme toutes les cartes mastercard  la carte lydia est acceptée partout dans le monde  par les commerçants et les distributeurs automatiques de billets  quelle que soit la monnaie du pays !   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                          tweet  \n",
      "25636  @megaviria Bonjour, La carte Lydia est une carte de paiement Mastercard conçue par Lydia.  Comme toutes les cartes Mastercard, la carte Lydia est acceptée partout dans le monde, par les commerçants et les distributeurs automatiques de billets, quelle que soit la monnaie du pays !  \n",
      "True\n",
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(df_disparition['created_at'].head(15))\n",
    "print(df_contactless[['clean_tweet','tweet' ]].head(1))\n",
    "\n",
    "print(df_contactless.isnull().values.any())\n",
    "print(df_contactless.clean_tweet.isnull().values.any())\n",
    "df_contactless.created_at.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contactless.to_pickle(r\"/home/cash/output/cleaned_queries/without_geoloc/query_fr_contactless_cleaned.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_contactless_lecture = pd.read_pickle(r\"/home/cash/output/cleaned_queries/without_geoloc/query_fr_contactless_cleaned.pickle\")\n",
    "df_contactless.equals(df_contactless_lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48030, 42)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_contactless.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
